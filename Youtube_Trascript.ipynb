{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZcsrDW40sex37XfCHqkrM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srahul101082/Generic_Utils/blob/main/Youtube_Trascript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUJOo3MxlWT1"
      },
      "outputs": [],
      "source": [
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "transcript = YouTubeTranscriptApi.get_transcript(\"3G5hWM6jqPk\")\n",
        "#print(transcript)"
      ],
      "metadata": {
        "id": "AoY2dzbMlnyw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "def format_transcript(transcript, max_line_width=200):\n",
        "    formatted_transcript = \"\"\n",
        "    wrapper = textwrap.TextWrapper(width=max_line_width)\n",
        "    \n",
        "    for entry in transcript:\n",
        "        wrapped_text = wrapper.fill(text=entry['text'])\n",
        "        formatted_transcript += wrapped_text + \"\\n\\n\"\n",
        "    return formatted_transcript"
      ],
      "metadata": {
        "id": "7R-TmVxdnGFv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_transcript = format_transcript(transcript)\n",
        "print(formatted_transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPOyQTOIneaV",
        "outputId": "063814a8-719f-4037-8eeb-3903a6318a28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foreign\n",
            "\n",
            "I'm really really excited about this\n",
            "\n",
            "lecture because as Alexander introduced\n",
            "\n",
            "to yesterday\n",
            "\n",
            "right now we're in this tremendous age\n",
            "\n",
            "of generative Ai and today we're going\n",
            "\n",
            "to learn the foundations of deep\n",
            "\n",
            "generative modeling where we're going to\n",
            "\n",
            "talk about Building Systems that can not\n",
            "\n",
            "only look for patterns in data but can\n",
            "\n",
            "actually go A Step Beyond this to\n",
            "\n",
            "generate brand new data instances based\n",
            "\n",
            "on those learned patterns\n",
            "\n",
            "this is an incredibly complex and\n",
            "\n",
            "Powerful idea and as I mentioned it's a\n",
            "\n",
            "particular subset of deep learning that\n",
            "\n",
            "has actually really exploded in the past\n",
            "\n",
            "couple of years and this year in\n",
            "\n",
            "particular\n",
            "\n",
            "so to start and to demonstrate how\n",
            "\n",
            "powerful these algorithms are let me\n",
            "\n",
            "show you\n",
            "\n",
            "these three different faces\n",
            "\n",
            "I want you to take a minute think\n",
            "\n",
            "think about which face you think is real\n",
            "\n",
            "raise your hand if you think it's face a\n",
            "\n",
            "okay you see a couple of people face B\n",
            "\n",
            "many more people\n",
            "\n",
            "face C\n",
            "\n",
            "about second place\n",
            "\n",
            "well the truth is that all of you are\n",
            "\n",
            "wrong\n",
            "\n",
            "all three of these faces are fake\n",
            "\n",
            "these people do not exist these images\n",
            "\n",
            "were synthesized by Deep generative\n",
            "\n",
            "models trained on data of human faces\n",
            "\n",
            "and asked to produce new instances\n",
            "\n",
            "now I think that this demonstration kind\n",
            "\n",
            "of demonstrates the power of these ideas\n",
            "\n",
            "and the power of this notion of\n",
            "\n",
            "generative modeling so let's get a\n",
            "\n",
            "little more concrete about how we can\n",
            "\n",
            "formalize this\n",
            "\n",
            "so far in this course we've been looking\n",
            "\n",
            "at what we call problems of supervised\n",
            "\n",
            "learning meaning that we're given data\n",
            "\n",
            "and associated with that data is a set\n",
            "\n",
            "of labels\n",
            "\n",
            "our goal is to learn a function that\n",
            "\n",
            "maps that data to the labels\n",
            "\n",
            "now we're in a course on deep learning\n",
            "\n",
            "so we've been concerned with functional\n",
            "\n",
            "mappings that are defined by Deep neural\n",
            "\n",
            "networks but really that function could\n",
            "\n",
            "be anything neural networks are powerful\n",
            "\n",
            "but we could use other techniques as\n",
            "\n",
            "well\n",
            "\n",
            "in contrast there's another class of\n",
            "\n",
            "problems in machine learning that we\n",
            "\n",
            "refer to as unsupervised learning where\n",
            "\n",
            "we take data but now we're given only\n",
            "\n",
            "data no labels and our goal is to try to\n",
            "\n",
            "build some method that can understand\n",
            "\n",
            "the hidden underlying structure of that\n",
            "\n",
            "data\n",
            "\n",
            "what this allows us to do is it gives us\n",
            "\n",
            "new insights into the foundational\n",
            "\n",
            "representation of the data and as we'll\n",
            "\n",
            "see later\n",
            "\n",
            "actually enables us to generate new data\n",
            "\n",
            "instances\n",
            "\n",
            "now this class of problems this\n",
            "\n",
            "definition of unsupervised learning\n",
            "\n",
            "captures the types of models that we're\n",
            "\n",
            "going to talk about today in the focus\n",
            "\n",
            "on generative modeling\n",
            "\n",
            "which is an example of unsupervised\n",
            "\n",
            "learning and is United by this goal of\n",
            "\n",
            "the problem where we're given only\n",
            "\n",
            "samples from a training set\n",
            "\n",
            "and we want to learn a model that\n",
            "\n",
            "represents the distribution of the data\n",
            "\n",
            "that the model is seeing\n",
            "\n",
            "generative modeling takes two general\n",
            "\n",
            "forms\n",
            "\n",
            "first density estimation and second\n",
            "\n",
            "sample generation\n",
            "\n",
            "in density estimation the task is given\n",
            "\n",
            "some data examples\n",
            "\n",
            "our goal is to train a model that learns\n",
            "\n",
            "a underlying probability distribution\n",
            "\n",
            "that describes the where the data came\n",
            "\n",
            "from\n",
            "\n",
            "with sample generation the idea is\n",
            "\n",
            "similar but the focus is more on\n",
            "\n",
            "actually generating new instances\n",
            "\n",
            "our goal with sample generation is to\n",
            "\n",
            "again learn this model of this\n",
            "\n",
            "underlying probability distribution\n",
            "\n",
            "but then use that model to sample from\n",
            "\n",
            "it and generate new instances that are\n",
            "\n",
            "similar to the data that we've seen\n",
            "\n",
            "approximately falling along ideally that\n",
            "\n",
            "same real data distribution\n",
            "\n",
            "now in both these cases of density\n",
            "\n",
            "estimation and Sample generation\n",
            "\n",
            "the underlying question is the same\n",
            "\n",
            "our learning task is to try to build a\n",
            "\n",
            "model that learns this probability\n",
            "\n",
            "distribution that is as close as\n",
            "\n",
            "possible to the true data distribution\n",
            "\n",
            "okay so with this definition and this\n",
            "\n",
            "concept of generative modeling what are\n",
            "\n",
            "some ways that we can actually deploy\n",
            "\n",
            "generative modeling forward in the real\n",
            "\n",
            "world for high impact applications\n",
            "\n",
            "well part of the reason that generative\n",
            "\n",
            "models is are so powerful is that they\n",
            "\n",
            "have this ability to uncover the\n",
            "\n",
            "underlying features in a data set and\n",
            "\n",
            "encode it in an efficient way\n",
            "\n",
            "so for example if we're considering the\n",
            "\n",
            "problem of facial detection and we're\n",
            "\n",
            "given a data set with many many\n",
            "\n",
            "different faces\n",
            "\n",
            "starting out without inspecting this\n",
            "\n",
            "data we may not know what the\n",
            "\n",
            "distribution of Faces in this data set\n",
            "\n",
            "is with respect to Features we may be\n",
            "\n",
            "caring about for example the pose of the\n",
            "\n",
            "head clothing glasses skin tone Hair Etc\n",
            "\n",
            "and it can be the case that our training\n",
            "\n",
            "data may be very very biased towards\n",
            "\n",
            "particular features without us even\n",
            "\n",
            "realizing this\n",
            "\n",
            "using generative models we can actually\n",
            "\n",
            "identify the distributions of these\n",
            "\n",
            "underlying features in a completely\n",
            "\n",
            "automatic way without any labeling in\n",
            "\n",
            "order to understand what features may be\n",
            "\n",
            "overrepresented in the data what\n",
            "\n",
            "features may be underrepresented in the\n",
            "\n",
            "data\n",
            "\n",
            "and this is the focus of today and\n",
            "\n",
            "tomorrow's software Labs which are going\n",
            "\n",
            "to be part of the software lab\n",
            "\n",
            "competition developing generative models\n",
            "\n",
            "that can do this task and using it to\n",
            "\n",
            "uncover and diagnose biases that can\n",
            "\n",
            "exist within facial detection models\n",
            "\n",
            "another really powerful example is in\n",
            "\n",
            "the case of outlier detection\n",
            "\n",
            "identifying rare events so let's\n",
            "\n",
            "consider the example of self-driving\n",
            "\n",
            "autonomous cars\n",
            "\n",
            "with an autonomous car let's say it's\n",
            "\n",
            "driving out in the real world we really\n",
            "\n",
            "really want to make sure that that car\n",
            "\n",
            "can be able to handle all the possible\n",
            "\n",
            "scenarios and all the possible cases it\n",
            "\n",
            "may encounter including edge cases like\n",
            "\n",
            "a deer coming in front of the car or\n",
            "\n",
            "some unexpected rare events not just you\n",
            "\n",
            "know the typical straight freeway\n",
            "\n",
            "driving that it may see the majority of\n",
            "\n",
            "the time\n",
            "\n",
            "with generative models we can use this\n",
            "\n",
            "idea of density estimation to be able to\n",
            "\n",
            "identify rare and anomalous events\n",
            "\n",
            "within the training data and as they're\n",
            "\n",
            "occurring as the model sees them for the\n",
            "\n",
            "first time\n",
            "\n",
            "so hopefully this paints this picture of\n",
            "\n",
            "what generative modeling the underlying\n",
            "\n",
            "concept is and a couple of different\n",
            "\n",
            "ways in which we can actually deploy\n",
            "\n",
            "these ideas for powerful and impactful\n",
            "\n",
            "real world applications\n",
            "\n",
            "yeah\n",
            "\n",
            "in today's lecture we're going to focus\n",
            "\n",
            "on a broad class of generative models\n",
            "\n",
            "that we call latent variable models and\n",
            "\n",
            "specifically distilled down into two\n",
            "\n",
            "subtypes of latent variable models\n",
            "\n",
            "first things first I've introduced this\n",
            "\n",
            "term latent variable but I haven't told\n",
            "\n",
            "you or described to you what that\n",
            "\n",
            "actually is\n",
            "\n",
            "I think a great example and one of my\n",
            "\n",
            "favorite examples throughout this entire\n",
            "\n",
            "course\n",
            "\n",
            "that gets at this idea of the latent\n",
            "\n",
            "variable is this little story from\n",
            "\n",
            "Plato's Republic\n",
            "\n",
            "which is known as the myth of the cave\n",
            "\n",
            "in this myth there is a group of\n",
            "\n",
            "prisoners and as part of their\n",
            "\n",
            "punishment they're constrained to face a\n",
            "\n",
            "wall\n",
            "\n",
            "now the only things the prisoners can\n",
            "\n",
            "observe are shadows of objects that are\n",
            "\n",
            "passing in front of a fire that's behind\n",
            "\n",
            "them and they're observing the casting\n",
            "\n",
            "of the Shadows on the wall of this cave\n",
            "\n",
            "to the prisoners those Shadows are the\n",
            "\n",
            "only things they see their observations\n",
            "\n",
            "they can measure them they can give them\n",
            "\n",
            "names because to them that's their\n",
            "\n",
            "reality\n",
            "\n",
            "but they're unable to directly see the\n",
            "\n",
            "underlying objects the true factors\n",
            "\n",
            "themselves that are casting those\n",
            "\n",
            "Shadows\n",
            "\n",
            "those objects here are like latent\n",
            "\n",
            "variables in machine learning\n",
            "\n",
            "they're not directly observable\n",
            "\n",
            "but they're the true underlying features\n",
            "\n",
            "or explanatory factors that create the\n",
            "\n",
            "observed differences and variables that\n",
            "\n",
            "we can see and observe\n",
            "\n",
            "and this gets out the goal of generative\n",
            "\n",
            "modeling which is to find ways that we\n",
            "\n",
            "can actually learn these hidden features\n",
            "\n",
            "these underlying latent variables even\n",
            "\n",
            "when we're only given observations of\n",
            "\n",
            "The observed data\n",
            "\n",
            "so let's start by discussing a very\n",
            "\n",
            "simple generative model that tries to do\n",
            "\n",
            "this through the idea of encoding the\n",
            "\n",
            "data input\n",
            "\n",
            "the models we're going to talk about are\n",
            "\n",
            "called autoencoders\n",
            "\n",
            "and to take a look at how an auto\n",
            "\n",
            "encoder works we'll go through step by\n",
            "\n",
            "step starting with the first step of\n",
            "\n",
            "taking some raw input data and passing\n",
            "\n",
            "it through a series of neural network\n",
            "\n",
            "layers\n",
            "\n",
            "now the output of this of this first\n",
            "\n",
            "step is what we refer to as a low\n",
            "\n",
            "dimensional latent space\n",
            "\n",
            "it's an encoded representation of those\n",
            "\n",
            "underlying features\n",
            "\n",
            "and that's our goal in trying to train\n",
            "\n",
            "this model and predict those features\n",
            "\n",
            "the reason a model like this is called\n",
            "\n",
            "an encoder an autoencoder is that it's\n",
            "\n",
            "mapping the data X into this Vector of\n",
            "\n",
            "latent variables Z\n",
            "\n",
            "now let's ask ourselves the question\n",
            "\n",
            "let's pause for a moment\n",
            "\n",
            "why maybe we care about having this\n",
            "\n",
            "latent variable Vector Z be in a low\n",
            "\n",
            "dimensional space\n",
            "\n",
            "anyone have any ideas\n",
            "\n",
            "all right\n",
            "\n",
            "maybe there are some ideas as yes\n",
            "\n",
            "the suggestion was that it's more\n",
            "\n",
            "efficient yes that's that's gets at it\n",
            "\n",
            "the heart of the of the question the\n",
            "\n",
            "idea of having that low dimensional\n",
            "\n",
            "latent space is that it's a very\n",
            "\n",
            "efficient compact encoding of the rich\n",
            "\n",
            "High dimensional data that we may start\n",
            "\n",
            "with\n",
            "\n",
            "as you pointed out right what this means\n",
            "\n",
            "is that we're able to compress data into\n",
            "\n",
            "this small feature representation a\n",
            "\n",
            "vector\n",
            "\n",
            "that captures this compactness and\n",
            "\n",
            "richness without requiring so much\n",
            "\n",
            "memory or so much storage\n",
            "\n",
            "so how do we actually train the network\n",
            "\n",
            "to learn this latent variable vector\n",
            "\n",
            "since we don't have training data we\n",
            "\n",
            "can't explicitly observe these latent\n",
            "\n",
            "variables Z\n",
            "\n",
            "we need to do something more clever\n",
            "\n",
            "what the auto encoder does is it builds\n",
            "\n",
            "a way to decode this latent variable\n",
            "\n",
            "Vector back up to the original data\n",
            "\n",
            "space trying to reconstruct their\n",
            "\n",
            "original image from that compressed\n",
            "\n",
            "efficient latent encoding\n",
            "\n",
            "and once again we can use a series of\n",
            "\n",
            "neural network layers such as\n",
            "\n",
            "convolutional layers fully connected\n",
            "\n",
            "layers but now to map back from that\n",
            "\n",
            "lower dimensional space back upwards to\n",
            "\n",
            "the input space\n",
            "\n",
            "this generates a reconstructed output\n",
            "\n",
            "which we can denote as X hat since it's\n",
            "\n",
            "an imperfect reconstruction of our\n",
            "\n",
            "original input data\n",
            "\n",
            "to train this network\n",
            "\n",
            "all we have to do is compare the\n",
            "\n",
            "outputted Reconstruction and the\n",
            "\n",
            "original input data and say how do we\n",
            "\n",
            "make these as similar as possible we can\n",
            "\n",
            "minimize the distance between that input\n",
            "\n",
            "and our reconstructed output\n",
            "\n",
            "so for example for an image we can\n",
            "\n",
            "compare the pixel wise difference\n",
            "\n",
            "between the input data and the\n",
            "\n",
            "reconstructed output just subtracting\n",
            "\n",
            "the images from one another and squaring\n",
            "\n",
            "that difference to capture the pixel\n",
            "\n",
            "wise Divergence between the input and\n",
            "\n",
            "the reconstruction\n",
            "\n",
            "what I hope you'll notice and appreciate\n",
            "\n",
            "is in that definition of the loss\n",
            "\n",
            "it doesn't require any labels the only\n",
            "\n",
            "components of that loss are the original\n",
            "\n",
            "input data X and the reconstructed\n",
            "\n",
            "output X hat\n",
            "\n",
            "so\n",
            "\n",
            "I've simplified now this diagram by\n",
            "\n",
            "abstracting away those individual neural\n",
            "\n",
            "network layers in both the encoder and\n",
            "\n",
            "decoder components of this\n",
            "\n",
            "and again this idea of not requiring any\n",
            "\n",
            "labels gets back to the idea of\n",
            "\n",
            "unsupervised learning since what we've\n",
            "\n",
            "done is we've been able to learn a\n",
            "\n",
            "encoded quantity our latent variables\n",
            "\n",
            "that we cannot observe without any\n",
            "\n",
            "explicit labels all we started from was\n",
            "\n",
            "the raw data itself\n",
            "\n",
            "it turns out that as as the question and\n",
            "\n",
            "answer got at that dimensionality of the\n",
            "\n",
            "latent space has a huge impact on the\n",
            "\n",
            "quality of the generated reconstructions\n",
            "\n",
            "and how compressed that information\n",
            "\n",
            "bottleneck is\n",
            "\n",
            "Auto encoding is a form of compression\n",
            "\n",
            "and so the lower the dimensionality of\n",
            "\n",
            "the latent space the less good our\n",
            "\n",
            "reconstructions are going to be but the\n",
            "\n",
            "higher the dimensionality the more the\n",
            "\n",
            "less efficient that encoding is going to\n",
            "\n",
            "be\n",
            "\n",
            "so to summarize this this first part\n",
            "\n",
            "this idea of an autoencoder is using\n",
            "\n",
            "this bottlenecked compressed hidden\n",
            "\n",
            "latent layer to try to bring the network\n",
            "\n",
            "down to learn a compact efficient\n",
            "\n",
            "representation of the data\n",
            "\n",
            "we don't require any labels this is\n",
            "\n",
            "completely unsupervised and so in this\n",
            "\n",
            "way we're able to automatically encode\n",
            "\n",
            "information\n",
            "\n",
            "within the data itself to learn this\n",
            "\n",
            "latent space\n",
            "\n",
            "Auto encoding information Auto encoding\n",
            "\n",
            "data\n",
            "\n",
            "now this is of the pretty simple model\n",
            "\n",
            "and it turns out that in practice this\n",
            "\n",
            "idea of self-encoding or Auto encoding\n",
            "\n",
            "has a bit of a Twist on it to allow us\n",
            "\n",
            "to actually generate new examples that\n",
            "\n",
            "are not only reconstructions of the\n",
            "\n",
            "input data itself\n",
            "\n",
            "and this leads us to the concept of\n",
            "\n",
            "variational autoencoders or vaes\n",
            "\n",
            "with the traditional autoencoder that we\n",
            "\n",
            "just saw\n",
            "\n",
            "if we pay closer attention to the latent\n",
            "\n",
            "layer right which is shown in that\n",
            "\n",
            "orange salmon color\n",
            "\n",
            "that latent layer is just a normal layer\n",
            "\n",
            "in the neural network it's completely\n",
            "\n",
            "deterministic what that means is once\n",
            "\n",
            "we've trained the network once the\n",
            "\n",
            "weights are set anytime we pass a given\n",
            "\n",
            "input in and go back through the latent\n",
            "\n",
            "layer decode back out we're going to get\n",
            "\n",
            "the same exact reconstruction the\n",
            "\n",
            "weights aren't changing it's\n",
            "\n",
            "deterministic\n",
            "\n",
            "in contrast\n",
            "\n",
            "variational autoencoders vaes introduce\n",
            "\n",
            "a element of Randomness a probabilistic\n",
            "\n",
            "Twist on this idea of Auto encoding what\n",
            "\n",
            "this will allow us to do is to actually\n",
            "\n",
            "generate new images\n",
            "\n",
            "similar to the or new data instances\n",
            "\n",
            "that are similar to the input data but\n",
            "\n",
            "not forced to be strict reconstructions\n",
            "\n",
            "in practice with the variational\n",
            "\n",
            "autoencoder we've replaced that single\n",
            "\n",
            "deterministic layer with a random\n",
            "\n",
            "sampling operation\n",
            "\n",
            "now instead of learning just the latent\n",
            "\n",
            "variables directly themselves for each\n",
            "\n",
            "latent variable we Define a mean and a\n",
            "\n",
            "standard deviation that captures a\n",
            "\n",
            "probability distribution over that\n",
            "\n",
            "latent variable\n",
            "\n",
            "what we've done is we've gone from a\n",
            "\n",
            "single Vector of latent variable Z to a\n",
            "\n",
            "vector of means mu and a vector of\n",
            "\n",
            "standard deviations Sigma that\n",
            "\n",
            "parametrize the probability\n",
            "\n",
            "distributions around those latent\n",
            "\n",
            "variables\n",
            "\n",
            "what this will allow us to do is Now\n",
            "\n",
            "sample using this element of Randomness\n",
            "\n",
            "this element of probability to then\n",
            "\n",
            "obtain a probabilistic representation of\n",
            "\n",
            "the latent space itself\n",
            "\n",
            "as you hopefully can tell right this is\n",
            "\n",
            "very very very similar to the\n",
            "\n",
            "autoencoder itself but we've just added\n",
            "\n",
            "this probabilistic twist where we can\n",
            "\n",
            "sample in that intermediate space to get\n",
            "\n",
            "these samples of latent variables\n",
            "\n",
            "foreign\n",
            "\n",
            "now to get a little more into the depth\n",
            "\n",
            "of how this is actually learned how this\n",
            "\n",
            "is actually trained\n",
            "\n",
            "with defining the vae we've eliminated\n",
            "\n",
            "this deterministic nature to now have\n",
            "\n",
            "these encoders and decoders that are\n",
            "\n",
            "probabilistic\n",
            "\n",
            "the encoder is Computing a probability\n",
            "\n",
            "distribution of the latent variable Z\n",
            "\n",
            "given input data X\n",
            "\n",
            "while the decoder is doing the inverse\n",
            "\n",
            "trying to learn a probability\n",
            "\n",
            "distribution\n",
            "\n",
            "back in the input data space given the\n",
            "\n",
            "latent variables Z\n",
            "\n",
            "and we Define separate sets of weights\n",
            "\n",
            "Phi and Theta to define the network\n",
            "\n",
            "weights for the encoder and decoder\n",
            "\n",
            "components of the vae\n",
            "\n",
            "all right so when we get now to how we\n",
            "\n",
            "actually optimize and learn the network\n",
            "\n",
            "weights in the vae\n",
            "\n",
            "first step is to define a loss function\n",
            "\n",
            "right that's the core element to\n",
            "\n",
            "training a neural network\n",
            "\n",
            "our loss is going to be a function of\n",
            "\n",
            "the data and a function of the neural\n",
            "\n",
            "network weights just like before\n",
            "\n",
            "but we have these two components these\n",
            "\n",
            "two terms that Define our vae loss\n",
            "\n",
            "first we see the Reconstruction loss\n",
            "\n",
            "just like before where the goal is to\n",
            "\n",
            "capture the difference between our input\n",
            "\n",
            "data and the reconstructed output\n",
            "\n",
            "and now for the vae we've introduced a\n",
            "\n",
            "second term to the loss what we call the\n",
            "\n",
            "regularization term\n",
            "\n",
            "often you'll maybe even see this\n",
            "\n",
            "referred to as a vae loss\n",
            "\n",
            "and we'll go into we'll go into\n",
            "\n",
            "describing what this regular\n",
            "\n",
            "regularization term means and what it's\n",
            "\n",
            "doing\n",
            "\n",
            "to do that and to understand remember\n",
            "\n",
            "and and keep in mind that in all neural\n",
            "\n",
            "network operations our goal is to try to\n",
            "\n",
            "optimize the network weights with\n",
            "\n",
            "respect to the data with respect to\n",
            "\n",
            "minimizing this objective loss\n",
            "\n",
            "and so here we're concerned with the\n",
            "\n",
            "network weights Phi and Theta that\n",
            "\n",
            "Define the weights of the encoder and\n",
            "\n",
            "the decoder\n",
            "\n",
            "we consider these two terms first the\n",
            "\n",
            "Reconstruction loss again the\n",
            "\n",
            "Reconstruction loss is very very similar\n",
            "\n",
            "same as before you can think of it as\n",
            "\n",
            "the error or the likelihood that\n",
            "\n",
            "effectively captures the difference\n",
            "\n",
            "between your input and your outputs and\n",
            "\n",
            "again we can trade this in an\n",
            "\n",
            "unsupervised way not requiring any\n",
            "\n",
            "labels to force the latent space and the\n",
            "\n",
            "network to learn how to effectively\n",
            "\n",
            "reconstruct the input data\n",
            "\n",
            "the second term the regularization term\n",
            "\n",
            "is now where things get a bit more\n",
            "\n",
            "interesting so let's go on on into this\n",
            "\n",
            "in a little bit more detail\n",
            "\n",
            "because we have this probability\n",
            "\n",
            "distribution\n",
            "\n",
            "and we're trying to compute this\n",
            "\n",
            "encoding and then decode back up\n",
            "\n",
            "as part of regular regularizing\n",
            "\n",
            "we want to take that inference over the\n",
            "\n",
            "latent distribution and constrain it to\n",
            "\n",
            "behave nicely if you will\n",
            "\n",
            "the way we do that is we place what we\n",
            "\n",
            "call a prior on the latent distribution\n",
            "\n",
            "and what this is is some initial\n",
            "\n",
            "hypothesis or guess about what that\n",
            "\n",
            "latent variable space may look like\n",
            "\n",
            "this helps us and helps the network to\n",
            "\n",
            "enforce a latent space that roughly\n",
            "\n",
            "tries to follow this prior distribution\n",
            "\n",
            "and this prior is denoted as P of Z\n",
            "\n",
            "right\n",
            "\n",
            "that term d That's effectively the\n",
            "\n",
            "regularization term it's capturing a\n",
            "\n",
            "distance between our encoding of the\n",
            "\n",
            "latent variables and our prior\n",
            "\n",
            "hypothesis about what the structure of\n",
            "\n",
            "that latent space should look like\n",
            "\n",
            "so over the course of training we're\n",
            "\n",
            "trying to enforce that each of those\n",
            "\n",
            "latent variables adapts a problem adopts\n",
            "\n",
            "a probability distribution that's\n",
            "\n",
            "similar to that prior\n",
            "\n",
            "a common Choice when training va's and\n",
            "\n",
            "developing these models is to enforce\n",
            "\n",
            "the latent variables to be roughly\n",
            "\n",
            "standard normal gaussian distributions\n",
            "\n",
            "meaning that they are centered around\n",
            "\n",
            "mean zero and they have a standard\n",
            "\n",
            "deviation of one\n",
            "\n",
            "what this allows us to do is to\n",
            "\n",
            "encourage the encoder to put the latent\n",
            "\n",
            "variables roughly around a centered\n",
            "\n",
            "space Distributing the encoding smoothly\n",
            "\n",
            "so that we don't get too much Divergence\n",
            "\n",
            "away from that smooth space\n",
            "\n",
            "which can occur if the network tries to\n",
            "\n",
            "cheat and try to Simply memorize the\n",
            "\n",
            "data\n",
            "\n",
            "by placing the gaussian standard normal\n",
            "\n",
            "prior on the latent space\n",
            "\n",
            "we can define a concrete mathematical\n",
            "\n",
            "term that captures the the distance the\n",
            "\n",
            "Divergence between our encoded latent\n",
            "\n",
            "variables and this prior\n",
            "\n",
            "and this is called the KL Divergence\n",
            "\n",
            "when our prior is a standard normal the\n",
            "\n",
            "KL Divergence takes the form of the\n",
            "\n",
            "equation that I'm showing up on the\n",
            "\n",
            "screen\n",
            "\n",
            "but what I want you to really get away\n",
            "\n",
            "Come Away with is that the concept of\n",
            "\n",
            "trying to smooth things out and to\n",
            "\n",
            "capture this Divergence and this\n",
            "\n",
            "difference between the prior and the\n",
            "\n",
            "latent encoding is all this KL term is\n",
            "\n",
            "trying to capture\n",
            "\n",
            "so it's a bit of math and I I\n",
            "\n",
            "acknowledge that but what I want to next\n",
            "\n",
            "go into is really what is the intuition\n",
            "\n",
            "behind this regularization operation\n",
            "\n",
            "why do we do this and why does the\n",
            "\n",
            "normal prior in particular work\n",
            "\n",
            "effectively for vaes\n",
            "\n",
            "so let's consider what properties we\n",
            "\n",
            "want our latent space to adopt and for\n",
            "\n",
            "this regularization to achieve\n",
            "\n",
            "the first is this goal of continuity\n",
            "\n",
            "we don't me and what we mean by\n",
            "\n",
            "continuity is that if there are points\n",
            "\n",
            "in the latent space that are close\n",
            "\n",
            "together\n",
            "\n",
            "ideally after decoding we should recover\n",
            "\n",
            "two reconstructions that are similar in\n",
            "\n",
            "content that make sense that they're\n",
            "\n",
            "close together\n",
            "\n",
            "the second key property is this idea of\n",
            "\n",
            "completeness\n",
            "\n",
            "we don't want there to be gaps in the\n",
            "\n",
            "lane space we want to be able to decode\n",
            "\n",
            "and sample from the latent space in a\n",
            "\n",
            "way that is smooth and a way that is\n",
            "\n",
            "connected\n",
            "\n",
            "to get more concrete\n",
            "\n",
            "what let's ask what could be the\n",
            "\n",
            "consequences of not regularizing our\n",
            "\n",
            "latent space at all\n",
            "\n",
            "well\n",
            "\n",
            "if we don't regularize we can end up\n",
            "\n",
            "with instances where there are points\n",
            "\n",
            "that are close in the latent space but\n",
            "\n",
            "don't end up with similar decodings or\n",
            "\n",
            "similar reconstructions\n",
            "\n",
            "similarly we could have points that\n",
            "\n",
            "don't lead to meaningful reconstructions\n",
            "\n",
            "at all they're somehow encoded but we\n",
            "\n",
            "can't decode effectively\n",
            "\n",
            "regularization allows us to realize\n",
            "\n",
            "points that end up close in the latent\n",
            "\n",
            "space and also are similarly\n",
            "\n",
            "reconstructed and meaningfully\n",
            "\n",
            "reconstructed\n",
            "\n",
            "okay so continuing with this example the\n",
            "\n",
            "example that I showed there and I didn't\n",
            "\n",
            "get into details was showing these\n",
            "\n",
            "shapes these shapes of different colors\n",
            "\n",
            "and that we're trying to be encoded in\n",
            "\n",
            "some lower dimensional space\n",
            "\n",
            "with regularization\n",
            "\n",
            "we are able to achieve this by Trying to\n",
            "\n",
            "minimize that\n",
            "\n",
            "regularization term it's not sufficient\n",
            "\n",
            "to just employ the Reconstruction loss\n",
            "\n",
            "alone to achieve this continuity and\n",
            "\n",
            "this completeness\n",
            "\n",
            "because of the fact that without\n",
            "\n",
            "regularization just encoding and\n",
            "\n",
            "reconstructing does not guarantee the\n",
            "\n",
            "properties of continuity and\n",
            "\n",
            "completeness\n",
            "\n",
            "we overcome this\n",
            "\n",
            "these issues of having potentially\n",
            "\n",
            "pointed distributions having\n",
            "\n",
            "discontinuities having disparate means\n",
            "\n",
            "that could end up in the latent space\n",
            "\n",
            "without the effect of regularization\n",
            "\n",
            "we overcome this by now regularizing the\n",
            "\n",
            "mean and the variance of the encoded\n",
            "\n",
            "latent distributions according to that\n",
            "\n",
            "normal prior\n",
            "\n",
            "what this allows is for the Learned\n",
            "\n",
            "distributions of those latent variables\n",
            "\n",
            "to effectively overlap in the latent\n",
            "\n",
            "space because everything is regularized\n",
            "\n",
            "to have according to this prior of mean\n",
            "\n",
            "zero standard deviation one\n",
            "\n",
            "and that centers the means\n",
            "\n",
            "regularizes the variances for each of\n",
            "\n",
            "those independent latent variable\n",
            "\n",
            "distributions\n",
            "\n",
            "together the effect of this\n",
            "\n",
            "regularization in net is that we can\n",
            "\n",
            "achieve continuity and completeness in\n",
            "\n",
            "the latent space\n",
            "\n",
            "points and distances that are close\n",
            "\n",
            "should correspond to similar\n",
            "\n",
            "reconstructions that we get out\n",
            "\n",
            "so hopefully this this gets at some of\n",
            "\n",
            "the intuition behind the idea of the vae\n",
            "\n",
            "behind the idea of the regularization\n",
            "\n",
            "and trying to enforce the structured\n",
            "\n",
            "normal prior on the latent space\n",
            "\n",
            "with this in hand with the two\n",
            "\n",
            "components of our loss function\n",
            "\n",
            "reconstructing the inputs regularizing\n",
            "\n",
            "learning to try to achieve continuity\n",
            "\n",
            "and completeness we can now think about\n",
            "\n",
            "how we Define a forward pass through the\n",
            "\n",
            "network going from an input example and\n",
            "\n",
            "being able to decode and sample from the\n",
            "\n",
            "latent variables to look at new examples\n",
            "\n",
            "our last critical step is how the actual\n",
            "\n",
            "back propagation training algorithm is\n",
            "\n",
            "defined and how we achieve this\n",
            "\n",
            "the key as I introduce with vaes is this\n",
            "\n",
            "notion of randomness of sampling that we\n",
            "\n",
            "have introduced by defining these\n",
            "\n",
            "probability distributions over each of\n",
            "\n",
            "the latent variables\n",
            "\n",
            "the problem this gives us is that we\n",
            "\n",
            "cannot back propagate directly through\n",
            "\n",
            "anything that has an element of sampling\n",
            "\n",
            "anything that has an element of\n",
            "\n",
            "randomness\n",
            "\n",
            "back propagation requires completely\n",
            "\n",
            "deterministic nodes deterministic layers\n",
            "\n",
            "to be able to successfully apply\n",
            "\n",
            "gradient descent and the back\n",
            "\n",
            "propagation algorithm\n",
            "\n",
            "the Breakthrough idea that enabled vaes\n",
            "\n",
            "to be trained completely end to end was\n",
            "\n",
            "this idea of re-parametrization\n",
            "\n",
            "within that sampling layer\n",
            "\n",
            "and I'll give you the key idea about how\n",
            "\n",
            "this operation works it's actually\n",
            "\n",
            "really quite quite clever\n",
            "\n",
            "so as I said when we have a notion of\n",
            "\n",
            "randomness of probability we can't\n",
            "\n",
            "sample directly through that layer\n",
            "\n",
            "instead with re-parametrization what we\n",
            "\n",
            "do is we redefine how a latent variable\n",
            "\n",
            "Vector is sampled as a sum of a fixed\n",
            "\n",
            "deterministic mean mu\n",
            "\n",
            "a fixed Vector of standard deviation\n",
            "\n",
            "Sigma and now the trick is that we\n",
            "\n",
            "divert all the randomness all the\n",
            "\n",
            "sampling to a random constant Epsilon\n",
            "\n",
            "that's drawn from a normal distribution\n",
            "\n",
            "so mean itself is fixed standard\n",
            "\n",
            "deviation is fixed all the randomness\n",
            "\n",
            "and the sampling occurs according to\n",
            "\n",
            "that Epsilon constant we can then scale\n",
            "\n",
            "the mean and standard deviation by that\n",
            "\n",
            "random constant to re-achieve the\n",
            "\n",
            "sampling operation within the latent\n",
            "\n",
            "variables themselves\n",
            "\n",
            "what this actually looks like and an\n",
            "\n",
            "illustration that breaks down this\n",
            "\n",
            "concept of re-parametrization and\n",
            "\n",
            "Divergence is as follows\n",
            "\n",
            "so look looking here right what I've\n",
            "\n",
            "shown is these completely deterministic\n",
            "\n",
            "steps in blue\n",
            "\n",
            "and the sampling random steps in Orange\n",
            "\n",
            "originally if our latent variables are\n",
            "\n",
            "what effectively are capturing the\n",
            "\n",
            "randomness the sampling themselves we\n",
            "\n",
            "have this problem in that we can't back\n",
            "\n",
            "propagate we can't train directly\n",
            "\n",
            "through anything that has stochasticity\n",
            "\n",
            "that has randomness\n",
            "\n",
            "what reparametrization allows us to do\n",
            "\n",
            "is it shifts this diagram where now\n",
            "\n",
            "we've completely diverted that sampling\n",
            "\n",
            "operation off to the side to this\n",
            "\n",
            "constant Epsilon which is drawn from a\n",
            "\n",
            "normal prior\n",
            "\n",
            "and now when we look back at our latent\n",
            "\n",
            "variable it is deterministic with\n",
            "\n",
            "respect to that sampling operation\n",
            "\n",
            "what this means is that we can back\n",
            "\n",
            "propagate to update our Network weights\n",
            "\n",
            "completely end to end\n",
            "\n",
            "without having to worry about direct\n",
            "\n",
            "Randomness direct stochasticity within\n",
            "\n",
            "those latent variables C\n",
            "\n",
            "this trick is really really powerful\n",
            "\n",
            "because it enabled the ability to train\n",
            "\n",
            "these va's completely end to end in a in\n",
            "\n",
            "through back propagation algorithm\n",
            "\n",
            "all right so at this point we've gone\n",
            "\n",
            "through the core architecture of vais\n",
            "\n",
            "we've introduced these two terms of the\n",
            "\n",
            "loss we've seen how we can train it end\n",
            "\n",
            "to end\n",
            "\n",
            "now let's consider what these latent\n",
            "\n",
            "variables are actually capturing and\n",
            "\n",
            "what they represent\n",
            "\n",
            "when we impose this distributional prior\n",
            "\n",
            "what it allows us to do is to sample\n",
            "\n",
            "effectively from the latent space and\n",
            "\n",
            "actually slowly perturb the value of\n",
            "\n",
            "single latent variables\n",
            "\n",
            "keeping the other ones fixed\n",
            "\n",
            "and what you can observe and what you\n",
            "\n",
            "can see here is that by doing that\n",
            "\n",
            "perturbation that tuning of the value of\n",
            "\n",
            "the latent variables\n",
            "\n",
            "we can run the decoder of the vae every\n",
            "\n",
            "time reconstruct the output every time\n",
            "\n",
            "we do that tuning\n",
            "\n",
            "and what you'll see hopefully with this\n",
            "\n",
            "example with the face is that an\n",
            "\n",
            "individual latent variable is capturing\n",
            "\n",
            "something semantically informative\n",
            "\n",
            "something meaningful and we see that by\n",
            "\n",
            "this perturbation by this tuning\n",
            "\n",
            "in this example the face as you\n",
            "\n",
            "hopefully can appreciate is Shifting the\n",
            "\n",
            "pose is Shifting and all this is driven\n",
            "\n",
            "by is the perturbation of a single\n",
            "\n",
            "latent variable\n",
            "\n",
            "tuning the value of that latent variable\n",
            "\n",
            "and seeing how that affects the decoded\n",
            "\n",
            "reconstruction\n",
            "\n",
            "the network is actually able to learn\n",
            "\n",
            "these different encoded features these\n",
            "\n",
            "different latent variables\n",
            "\n",
            "such that by perturbing the values of\n",
            "\n",
            "them individually we can interpret and\n",
            "\n",
            "make sense of what those latent\n",
            "\n",
            "variables mean and what they represent\n",
            "\n",
            "to make this more concrete right\n",
            "\n",
            "we can consider even multiple latent\n",
            "\n",
            "variables simultaneously compare one\n",
            "\n",
            "against the other and ideally we want\n",
            "\n",
            "those latent features to be as\n",
            "\n",
            "independent as possible in order to get\n",
            "\n",
            "at the most compact and richest\n",
            "\n",
            "representation and compact encoding so\n",
            "\n",
            "here again in this example of faces\n",
            "\n",
            "we're walking along two axes\n",
            "\n",
            "head pose on the x-axis and what appears\n",
            "\n",
            "to be kind of a notion of a smile on the\n",
            "\n",
            "y-axis\n",
            "\n",
            "and you can see that with these\n",
            "\n",
            "reconstructions we can actually perturb\n",
            "\n",
            "these features to be able to perturb the\n",
            "\n",
            "end effect in the reconstructed space\n",
            "\n",
            "and so ultimately with with the vae our\n",
            "\n",
            "goal is to try to enforce as much\n",
            "\n",
            "information to be captured in that\n",
            "\n",
            "encoding as possible we want these\n",
            "\n",
            "latent features to be independent and\n",
            "\n",
            "ideally disentangled\n",
            "\n",
            "it turns out that there is a very uh\n",
            "\n",
            "clever and simple way to try to\n",
            "\n",
            "encourage this Independence and this\n",
            "\n",
            "disentanglement\n",
            "\n",
            "while this may look a little complicated\n",
            "\n",
            "with with the math and and a bit scary I\n",
            "\n",
            "will break this down with the idea of\n",
            "\n",
            "how a very simple concept enforces this\n",
            "\n",
            "independent latent encoding and this\n",
            "\n",
            "disentanglement\n",
            "\n",
            "all this term is showing is those two\n",
            "\n",
            "components of the loss the\n",
            "\n",
            "Reconstruction term the regularization\n",
            "\n",
            "term that's what I want you to focus on\n",
            "\n",
            "the idea of latent space disentanglement\n",
            "\n",
            "really arose with this concept of beta\n",
            "\n",
            "beta vaes\n",
            "\n",
            "what beta vas do is they introduce this\n",
            "\n",
            "parameter beta and what it is it's a\n",
            "\n",
            "weighting constant the weighting\n",
            "\n",
            "constant controls how powerful that\n",
            "\n",
            "regularization term is in the overall\n",
            "\n",
            "loss of the vae\n",
            "\n",
            "and it turns out that by increasing the\n",
            "\n",
            "value of beta you can try to encourage\n",
            "\n",
            "greater disentanglement more efficient\n",
            "\n",
            "encoding to enforce these latent\n",
            "\n",
            "variables to be uncorrelated with each\n",
            "\n",
            "other\n",
            "\n",
            "now if you're interested in\n",
            "\n",
            "mathematically why beta vas enforce this\n",
            "\n",
            "disentanglement there are many papers in\n",
            "\n",
            "the literature and proofs and\n",
            "\n",
            "discussions as to why this occurs and we\n",
            "\n",
            "can point you in those directions but to\n",
            "\n",
            "get a sense of what this actually\n",
            "\n",
            "affects Downstream when we look at face\n",
            "\n",
            "reconstruction as a task of Interest\n",
            "\n",
            "with the standard vae no beta term or\n",
            "\n",
            "rather a beta of one\n",
            "\n",
            "you can hopefully appreciate that the\n",
            "\n",
            "features of the rotation of the head the\n",
            "\n",
            "pose and the the rotation of the head is\n",
            "\n",
            "also actually ends up being correlated\n",
            "\n",
            "with smile and the facial the mouth\n",
            "\n",
            "expression in the mouth position\n",
            "\n",
            "in that as the head pose is changing the\n",
            "\n",
            "apparent smile or the position of the\n",
            "\n",
            "mouth is also changing\n",
            "\n",
            "but with beta vaes\n",
            "\n",
            "empirically we can observe that with\n",
            "\n",
            "imposing these beta values much much\n",
            "\n",
            "much greater than one we can try to\n",
            "\n",
            "enforce greater disentanglement where\n",
            "\n",
            "now we can consider only a single latent\n",
            "\n",
            "variable head pose and the smile the\n",
            "\n",
            "position of the mouth in these images is\n",
            "\n",
            "more constant compared to the standard\n",
            "\n",
            "vae\n",
            "\n",
            "all right so this is really all the core\n",
            "\n",
            "math the core operations the core\n",
            "\n",
            "architecture of the A's that we're going\n",
            "\n",
            "to cover in today's lecture and in this\n",
            "\n",
            "class in general\n",
            "\n",
            "to close this section and as a final\n",
            "\n",
            "note I want to remind you back to the\n",
            "\n",
            "motivating example that I introduced at\n",
            "\n",
            "the beginning of this lecture facial\n",
            "\n",
            "detection\n",
            "\n",
            "where now hopefully you've understood\n",
            "\n",
            "this concept of latent variable learning\n",
            "\n",
            "and encoding and how this may be useful\n",
            "\n",
            "for a task like facial detection where\n",
            "\n",
            "we may want to learn those distributions\n",
            "\n",
            "of the underlying features in the data\n",
            "\n",
            "and indeed you're going to get Hands-On\n",
            "\n",
            "practice in the software labs to build\n",
            "\n",
            "variational autoencoders that can\n",
            "\n",
            "automatically uncover\n",
            "\n",
            "features underlying facial detection\n",
            "\n",
            "data sets and use this to actually\n",
            "\n",
            "understand underlying and hidden biases\n",
            "\n",
            "that may exist with those data and with\n",
            "\n",
            "those models\n",
            "\n",
            "and it doesn't just stop there tomorrow\n",
            "\n",
            "we'll have a very very exciting guest\n",
            "\n",
            "lecture on robust and trustworthy deep\n",
            "\n",
            "learning which will take this concept A\n",
            "\n",
            "step further to realize how we can use\n",
            "\n",
            "this idea of generative models and\n",
            "\n",
            "latent variable learning to not only\n",
            "\n",
            "uncover and diagnose biases but actually\n",
            "\n",
            "solve and mitigate some of those harmful\n",
            "\n",
            "effects of those biases in neural\n",
            "\n",
            "networks for facial detection and other\n",
            "\n",
            "applications\n",
            "\n",
            "all right so to summarize quickly the\n",
            "\n",
            "key points of vais we've gone through\n",
            "\n",
            "how they're able to compress data into\n",
            "\n",
            "this compact encoded representation\n",
            "\n",
            "from this representation we can generate\n",
            "\n",
            "reconstructions of the input in a\n",
            "\n",
            "completely unsupervised fashion\n",
            "\n",
            "we can train them end to end using the\n",
            "\n",
            "repair maturization trick\n",
            "\n",
            "we can understand the semantic uh\n",
            "\n",
            "interpretation of individual latent\n",
            "\n",
            "variables by perturbing their values\n",
            "\n",
            "and finally we can sample from the\n",
            "\n",
            "latent space to generate new examples by\n",
            "\n",
            "passing back up through the decoder\n",
            "\n",
            "so vaes are looking at this idea of\n",
            "\n",
            "latent variable encoding and density\n",
            "\n",
            "estimation as their core problem\n",
            "\n",
            "what if now we only focus on the quality\n",
            "\n",
            "of the generated samples and that's the\n",
            "\n",
            "task that we care more about\n",
            "\n",
            "for that we're going to transition to a\n",
            "\n",
            "new type of generative model called a\n",
            "\n",
            "generative adversarial Network or Gam\n",
            "\n",
            "where with cans our goal is really that\n",
            "\n",
            "we care more about how well we generate\n",
            "\n",
            "new instances that are similar to the\n",
            "\n",
            "existing data meaning that we want to\n",
            "\n",
            "try to sample from a potentially very\n",
            "\n",
            "complex distribution that the model is\n",
            "\n",
            "trying to approximate\n",
            "\n",
            "it can be extremely extremely difficult\n",
            "\n",
            "to learn that distribution directly\n",
            "\n",
            "because it's complex it's high\n",
            "\n",
            "dimensional and we want to be able to\n",
            "\n",
            "get around that complexity\n",
            "\n",
            "what Gans do is they say okay what if we\n",
            "\n",
            "start from something super super simple\n",
            "\n",
            "as simple as it can get completely\n",
            "\n",
            "random noise\n",
            "\n",
            "could we build a neural network\n",
            "\n",
            "architecture that can learn to generate\n",
            "\n",
            "synthetic examples from complete random\n",
            "\n",
            "noise\n",
            "\n",
            "and this is the underlying concept of\n",
            "\n",
            "Gans\n",
            "\n",
            "where the goal is to train this\n",
            "\n",
            "generator Network that learns a\n",
            "\n",
            "transformation from noise to the\n",
            "\n",
            "training data distribution\n",
            "\n",
            "with the goal of making the generated\n",
            "\n",
            "examples as close to the real deal as\n",
            "\n",
            "possible\n",
            "\n",
            "with scans\n",
            "\n",
            "the Breakthrough idea here was to\n",
            "\n",
            "interface these two neural networks\n",
            "\n",
            "together\n",
            "\n",
            "one being a generator and one being a\n",
            "\n",
            "discriminator\n",
            "\n",
            "and these two components the generator\n",
            "\n",
            "and discriminator are at War at\n",
            "\n",
            "competition with each other\n",
            "\n",
            "specifically the goal of the generator\n",
            "\n",
            "network is to look at random noise and\n",
            "\n",
            "try to produce an imitation of the data\n",
            "\n",
            "that's as close to real as possible\n",
            "\n",
            "the discriminator that then takes the\n",
            "\n",
            "output of the generator as well as some\n",
            "\n",
            "real data examples and tries to learn a\n",
            "\n",
            "classification classification decision\n",
            "\n",
            "distinguishing real from fake\n",
            "\n",
            "and effectively in the Gan these two\n",
            "\n",
            "components are going back and forth\n",
            "\n",
            "competing each other\n",
            "\n",
            "trying to force the discriminator to\n",
            "\n",
            "better learn this distinction between\n",
            "\n",
            "real and fake while the generator is\n",
            "\n",
            "trying to fool and outperform the\n",
            "\n",
            "ability of the discriminator to make\n",
            "\n",
            "that classification\n",
            "\n",
            "so that's the overlying concept but what\n",
            "\n",
            "I'm really excited about is the next\n",
            "\n",
            "example which is one of my absolute\n",
            "\n",
            "favorite illustrations and walkthroughs\n",
            "\n",
            "in this class and it gets at the\n",
            "\n",
            "intuition behind Gans how they work and\n",
            "\n",
            "the underlying concept\n",
            "\n",
            "okay\n",
            "\n",
            "we're going to look at a 1D example\n",
            "\n",
            "points on a line right that's the data\n",
            "\n",
            "that we're working with\n",
            "\n",
            "and again the generator starts from\n",
            "\n",
            "random noise produces some fake data\n",
            "\n",
            "they're going to fall somewhere on this\n",
            "\n",
            "one-dimensional line\n",
            "\n",
            "now the next step is the discriminator\n",
            "\n",
            "then sees these points\n",
            "\n",
            "and it also sees some real data\n",
            "\n",
            "the goal of the discriminator is to be\n",
            "\n",
            "trained to Output a probability that a\n",
            "\n",
            "instance it sees is real or fake\n",
            "\n",
            "and initially in the beginning before\n",
            "\n",
            "training it's not trained right so its\n",
            "\n",
            "predictions may not be very good\n",
            "\n",
            "but over the course of training you're\n",
            "\n",
            "going to train it and it hopefully will\n",
            "\n",
            "start increasing the probability\n",
            "\n",
            "for those examples that are real and\n",
            "\n",
            "decreasing the probability for those\n",
            "\n",
            "examples that are fake\n",
            "\n",
            "overall goal is to predict what is real\n",
            "\n",
            "until\n",
            "\n",
            "eventually the discriminator reaches\n",
            "\n",
            "this point where it has a perfect\n",
            "\n",
            "separation perfect classification of\n",
            "\n",
            "real versus fake\n",
            "\n",
            "so at this point the discriminator\n",
            "\n",
            "thinks okay I've done my job now we go\n",
            "\n",
            "back to the generator\n",
            "\n",
            "and it sees the examples of where the\n",
            "\n",
            "real data lie\n",
            "\n",
            "and it can be forced to start moving its\n",
            "\n",
            "generated fake data closer and closer\n",
            "\n",
            "increasingly closer to the real data\n",
            "\n",
            "we can then go back to the discriminator\n",
            "\n",
            "which receives these newly synthesized\n",
            "\n",
            "examples from the generator and repeats\n",
            "\n",
            "that same process of estimating the\n",
            "\n",
            "probability that any given point is real\n",
            "\n",
            "and learning to increase the probability\n",
            "\n",
            "of the true real examples decrease the\n",
            "\n",
            "probability of the fake points\n",
            "\n",
            "adjusting adjusting over the course of\n",
            "\n",
            "its training\n",
            "\n",
            "and finally we can go back and repeat to\n",
            "\n",
            "the generator again one last time the\n",
            "\n",
            "generator starts moving those fake\n",
            "\n",
            "points closer\n",
            "\n",
            "closer and closer to the real data such\n",
            "\n",
            "that the fake data is almost following\n",
            "\n",
            "the distribution of the real data\n",
            "\n",
            "at this point it becomes very very hard\n",
            "\n",
            "for the discriminator to distinguish\n",
            "\n",
            "between what is real and what is fake\n",
            "\n",
            "while the generator will continue to try\n",
            "\n",
            "to create fake data points to fool the\n",
            "\n",
            "discriminator\n",
            "\n",
            "this is really the key concept the\n",
            "\n",
            "underlying intuition behind how the\n",
            "\n",
            "components of the Gan are essentially\n",
            "\n",
            "competing with each other going back and\n",
            "\n",
            "forth between the generator and the\n",
            "\n",
            "discriminator\n",
            "\n",
            "and in fact this is the this intuitive\n",
            "\n",
            "concept is how the Gan is trained in\n",
            "\n",
            "practice\n",
            "\n",
            "where the generator first tries to\n",
            "\n",
            "synthesize new examples synthetic\n",
            "\n",
            "examples to fool the discriminator and\n",
            "\n",
            "the goal of the discriminator is to take\n",
            "\n",
            "both the fake examples and the real data\n",
            "\n",
            "to try to identify the synthesized\n",
            "\n",
            "instances\n",
            "\n",
            "in training what this means is that the\n",
            "\n",
            "objective\n",
            "\n",
            "the loss for the generator and\n",
            "\n",
            "discriminator have to be at odds with\n",
            "\n",
            "each other they're adversarial and that\n",
            "\n",
            "is what gives rise to the component of\n",
            "\n",
            "adversarial ingenerative adversarial\n",
            "\n",
            "Network\n",
            "\n",
            "these adversarial objectives are then\n",
            "\n",
            "put together to then Define what it\n",
            "\n",
            "means to arrive at a stable Global\n",
            "\n",
            "Optimum where the generator is capable\n",
            "\n",
            "of producing the true data distribution\n",
            "\n",
            "that would completely fool the\n",
            "\n",
            "discriminator\n",
            "\n",
            "concretely this can be defined\n",
            "\n",
            "mathematically in terms of a loss\n",
            "\n",
            "objective and again\n",
            "\n",
            "though I'm I'm showing math I can we can\n",
            "\n",
            "distill this down and go through what\n",
            "\n",
            "each of these terms reflect in terms of\n",
            "\n",
            "that core intuitive idea and conceptual\n",
            "\n",
            "idea that hopefully that 1D example\n",
            "\n",
            "conveyed\n",
            "\n",
            "so we'll first consider\n",
            "\n",
            "the perspective of the discriminator D\n",
            "\n",
            "its goal is to maximize probability that\n",
            "\n",
            "its decisions uh in its decisions that\n",
            "\n",
            "real data are classified real Faith data\n",
            "\n",
            "classified as fake\n",
            "\n",
            "so here the first term\n",
            "\n",
            "G of Z is the generator's output and D\n",
            "\n",
            "of G of Z is the discriminator's\n",
            "\n",
            "estimate of that generated output as\n",
            "\n",
            "being fake\n",
            "\n",
            "D of x x is the real data and so D of X\n",
            "\n",
            "is the estimate of the probability that\n",
            "\n",
            "a real instance is fake 1 minus D of X\n",
            "\n",
            "is the estimate that that real instance\n",
            "\n",
            "is real\n",
            "\n",
            "so here in both these cases the\n",
            "\n",
            "discriminator is producing a decision\n",
            "\n",
            "about fake data real data and together\n",
            "\n",
            "it wants to try to maximize the\n",
            "\n",
            "probability that it's getting answers\n",
            "\n",
            "correct right\n",
            "\n",
            "now with the generator we have those\n",
            "\n",
            "same exact terms but keep in mind the\n",
            "\n",
            "generator is never\n",
            "\n",
            "able to affect anything the the\n",
            "\n",
            "discriminator's decision is actually\n",
            "\n",
            "doing besides generating new data\n",
            "\n",
            "examples\n",
            "\n",
            "so for the generator its objective is\n",
            "\n",
            "simply to minimize the probability that\n",
            "\n",
            "the generated data is identified as fake\n",
            "\n",
            "together we want to then put this\n",
            "\n",
            "together to Define what it means for the\n",
            "\n",
            "generator to synthesize fake images that\n",
            "\n",
            "hopefully fool the discriminator\n",
            "\n",
            "all in all right this term besides the\n",
            "\n",
            "math besides the particularities of this\n",
            "\n",
            "definition what I want you to get away\n",
            "\n",
            "from this from this section on Gans\n",
            "\n",
            "is that we have this dual competing\n",
            "\n",
            "objective where the generator is trying\n",
            "\n",
            "to synthesize these synthetic examples\n",
            "\n",
            "that ideally fool the best discriminator\n",
            "\n",
            "possible\n",
            "\n",
            "and in doing so the goal is to build up\n",
            "\n",
            "a network\n",
            "\n",
            "via this adversarial training this\n",
            "\n",
            "adversarial competition to use the\n",
            "\n",
            "generator to create new data that best\n",
            "\n",
            "mimics the true data distribution and is\n",
            "\n",
            "completely synthetic new instances\n",
            "\n",
            "foreign\n",
            "\n",
            "what this amounts to in practice is that\n",
            "\n",
            "after the training process you can look\n",
            "\n",
            "exclusively at the generator component\n",
            "\n",
            "and use it to then create new data\n",
            "\n",
            "instances\n",
            "\n",
            "all this is done by starting from random\n",
            "\n",
            "noise and trying to learn a model that\n",
            "\n",
            "goes from random noise to the real data\n",
            "\n",
            "distribution\n",
            "\n",
            "and effectively what Gans are doing is\n",
            "\n",
            "learning a function that transforms that\n",
            "\n",
            "distribution of random noise to some\n",
            "\n",
            "Target\n",
            "\n",
            "what this mapping does is it allows us\n",
            "\n",
            "to take a particular observation of\n",
            "\n",
            "noise in that noise space\n",
            "\n",
            "and map it to some output a particular\n",
            "\n",
            "output in our Target data space\n",
            "\n",
            "and in turn if we consider some other\n",
            "\n",
            "random sample of noise if we feed it\n",
            "\n",
            "through the generator again it's going\n",
            "\n",
            "to produce a completely new instance\n",
            "\n",
            "falling somewhere else on that true data\n",
            "\n",
            "distribution manifold\n",
            "\n",
            "and indeed what we can actually do is\n",
            "\n",
            "interpolate and Traverse between\n",
            "\n",
            "trajectories in the noise space that\n",
            "\n",
            "then map to traversals and and\n",
            "\n",
            "interpolations in the Target data space\n",
            "\n",
            "and this is really really cool because\n",
            "\n",
            "now you can think about an initial point\n",
            "\n",
            "and a Target point and all the steps\n",
            "\n",
            "that are going to take you to synthesize\n",
            "\n",
            "and and go between those images in that\n",
            "\n",
            "Target data distribution\n",
            "\n",
            "so hopefully this gets gives a sense of\n",
            "\n",
            "this concept of generative modeling for\n",
            "\n",
            "the purpose of creating new data\n",
            "\n",
            "instances\n",
            "\n",
            "and that notion of interpolation and\n",
            "\n",
            "data transformation leads very nicely to\n",
            "\n",
            "some of the recent advances and\n",
            "\n",
            "applications of Gans\n",
            "\n",
            "where one particularly commonly employed\n",
            "\n",
            "idea is to try to iteratively grow the\n",
            "\n",
            "Gan to get more and more detailed image\n",
            "\n",
            "Generations progressively adding layers\n",
            "\n",
            "over the course of training to then\n",
            "\n",
            "refine the examples generated by the\n",
            "\n",
            "generator\n",
            "\n",
            "and this is the approach that was used\n",
            "\n",
            "to generate those synthetic those images\n",
            "\n",
            "of those synthetic faces that I showed\n",
            "\n",
            "at the beginning of this lecture this\n",
            "\n",
            "idea of using again that is refined\n",
            "\n",
            "iteratively to produce higher resolution\n",
            "\n",
            "images\n",
            "\n",
            "another way we can extend this concept\n",
            "\n",
            "is to extend the Gan architecture to\n",
            "\n",
            "consider particular tasks and impose\n",
            "\n",
            "further structure on the networkers\n",
            "\n",
            "itself\n",
            "\n",
            "one particular idea is to say okay what\n",
            "\n",
            "if we have a particular label or some\n",
            "\n",
            "factor that we want to condition the\n",
            "\n",
            "generation on\n",
            "\n",
            "we call this C and it's supplied to both\n",
            "\n",
            "the generator and the discriminator\n",
            "\n",
            "what this will allow us to achieve is\n",
            "\n",
            "paired translation between different\n",
            "\n",
            "types of data so for example we can have\n",
            "\n",
            "images of a street view and we can have\n",
            "\n",
            "images of the segmentation of that\n",
            "\n",
            "street view and we can build a gan that\n",
            "\n",
            "can directly translate between the\n",
            "\n",
            "street view and the segmentation\n",
            "\n",
            "let's make this more concrete by\n",
            "\n",
            "considering some particular examples\n",
            "\n",
            "so what I just described was going from\n",
            "\n",
            "a segmentation label to a street scene\n",
            "\n",
            "we can also translate between a\n",
            "\n",
            "satellite view aerial satellite image to\n",
            "\n",
            "what is the road map equivalent of that\n",
            "\n",
            "aerial satellite image or a particular\n",
            "\n",
            "annotation or labels of the image of a\n",
            "\n",
            "building to the actual visual\n",
            "\n",
            "realization and visual facade of that\n",
            "\n",
            "building\n",
            "\n",
            "we can translate between different\n",
            "\n",
            "lighting conditions day to night\n",
            "\n",
            "black and white to color outlines to a\n",
            "\n",
            "colored photo\n",
            "\n",
            "all these cases and I think in\n",
            "\n",
            "particular the the most interesting and\n",
            "\n",
            "impactful to me is this translation\n",
            "\n",
            "between street view and aerial view and\n",
            "\n",
            "this is used to consider for example if\n",
            "\n",
            "you have data from Google Maps how you\n",
            "\n",
            "can go between a street view of the map\n",
            "\n",
            "to the aerial image of that\n",
            "\n",
            "finally again cons extending the same\n",
            "\n",
            "concept of translation bit between one\n",
            "\n",
            "domain to another\n",
            "\n",
            "idea is that of completely unpaired\n",
            "\n",
            "translation and this uses a particular\n",
            "\n",
            "Gan architecture called cyclogam\n",
            "\n",
            "and so in this video that I'm showing\n",
            "\n",
            "here the model takes as input a bunch of\n",
            "\n",
            "images in one domain and it doesn't\n",
            "\n",
            "necessarily have to have a corresponding\n",
            "\n",
            "image in another Target domain but it is\n",
            "\n",
            "trained to try to generate examples in\n",
            "\n",
            "that Target domain that roughly\n",
            "\n",
            "correspond to the source domain\n",
            "\n",
            "transferring the style of the source\n",
            "\n",
            "onto the Target and vice versa\n",
            "\n",
            "so this example is showing the\n",
            "\n",
            "translation of images in horse domain to\n",
            "\n",
            "zebra domain\n",
            "\n",
            "the concept here is this cyclic\n",
            "\n",
            "dependency right you have two Gans that\n",
            "\n",
            "are connected together via this cyclic\n",
            "\n",
            "loss transforming between one domain and\n",
            "\n",
            "another\n",
            "\n",
            "and really like all the examples that\n",
            "\n",
            "we've seen so far in this lecture the\n",
            "\n",
            "intuition is this idea of distribution\n",
            "\n",
            "transformation\n",
            "\n",
            "normally with again you're going from\n",
            "\n",
            "noise to some Target with the cycle Gan\n",
            "\n",
            "you're trying to go from some Source\n",
            "\n",
            "distribution some data manifold X to a\n",
            "\n",
            "target distribution another data\n",
            "\n",
            "manifold why\n",
            "\n",
            "and this is really really not only cool\n",
            "\n",
            "but also powerful in thinking about how\n",
            "\n",
            "we can translate across these different\n",
            "\n",
            "distributions flexibly\n",
            "\n",
            "and in fact this is a allows us to do\n",
            "\n",
            "Transformations not only to images but\n",
            "\n",
            "to speech and audio as well\n",
            "\n",
            "so in the case of speech and audio it\n",
            "\n",
            "turns out that you can take sound waves\n",
            "\n",
            "represent it\n",
            "\n",
            "compactly in a spectrogram image and use\n",
            "\n",
            "a cycle Gan to then translate and\n",
            "\n",
            "transform speech from one person's voice\n",
            "\n",
            "in one domain to another person's voice\n",
            "\n",
            "in another domain right these are two\n",
            "\n",
            "independent data distributions that we\n",
            "\n",
            "Define\n",
            "\n",
            "maybe you're getting a sense of where\n",
            "\n",
            "I'm hinting at maybe not but in fact\n",
            "\n",
            "this was exactly how we developed the\n",
            "\n",
            "model to synthesize the audio behind\n",
            "\n",
            "Obama's voice that we saw in yesterday's\n",
            "\n",
            "introductory lecture\n",
            "\n",
            "what we did was we trained a cycle Gan\n",
            "\n",
            "to take data in Alexander's voice\n",
            "\n",
            "and transform it into Data in the\n",
            "\n",
            "manifold of Obama's voice\n",
            "\n",
            "so we can visualize how that spectrogram\n",
            "\n",
            "waveform looks like for Alexander's\n",
            "\n",
            "Voice versus Obama's voice that was\n",
            "\n",
            "completely synthesized using this\n",
            "\n",
            "cyclegan approach\n",
            "\n",
            "hi everybody and welcome to my food\n",
            "\n",
            "sickness191\n",
            "\n",
            "official introductory course\n",
            "\n",
            "here\n",
            "\n",
            "at NYC\n",
            "\n",
            "hi everybody I replayed it okay but\n",
            "\n",
            "basically what we did was Alexander\n",
            "\n",
            "spoke that exact phrase that was played\n",
            "\n",
            "yesterday and we had the Train Cycle Gan\n",
            "\n",
            "model and we can deploy it then on that\n",
            "\n",
            "exact audio to transform it from the\n",
            "\n",
            "domain of Alexander's voice to Obama's\n",
            "\n",
            "voice generating the synthetic audio\n",
            "\n",
            "that was played for that video clip\n",
            "\n",
            "all right\n",
            "\n",
            "okay before I accidentally uh played\n",
            "\n",
            "again I\n",
            "\n",
            "jump now to the summary slide\n",
            "\n",
            "so today in this lecture we've learned\n",
            "\n",
            "deep generative models specifically\n",
            "\n",
            "talking mostly about latent variable\n",
            "\n",
            "models\n",
            "\n",
            "autoencoders variational Auto encoders\n",
            "\n",
            "where our goal is to learn this low\n",
            "\n",
            "dimensional latent encoding of the data\n",
            "\n",
            "as well as generative adversarial\n",
            "\n",
            "networks where we have these competing\n",
            "\n",
            "generator and discriminator components\n",
            "\n",
            "that are trying to synthesize synthetic\n",
            "\n",
            "examples\n",
            "\n",
            "we've talked about these core\n",
            "\n",
            "foundational generative methods but it\n",
            "\n",
            "turns out as I alluded to in the\n",
            "\n",
            "beginning of the lecture\n",
            "\n",
            "that in this past year in particular\n",
            "\n",
            "we've seen truly truly tremendous\n",
            "\n",
            "advances in generative modeling many of\n",
            "\n",
            "which have not been from those two\n",
            "\n",
            "methods those two foundational methods\n",
            "\n",
            "that we described\n",
            "\n",
            "but rather a new approach called\n",
            "\n",
            "diffusion modeling\n",
            "\n",
            "diffusion models are driving are the\n",
            "\n",
            "driving tools behind the tremendous\n",
            "\n",
            "advances in generative AI that we've\n",
            "\n",
            "seen in this past year in particular\n",
            "\n",
            "viez Gans they're learning these\n",
            "\n",
            "Transformations these encodings but\n",
            "\n",
            "they're largely restricted to generating\n",
            "\n",
            "examples that fall similar to the data\n",
            "\n",
            "space that they've seen before\n",
            "\n",
            "diffusion models have this ability to\n",
            "\n",
            "now hallucinate and envision and imagine\n",
            "\n",
            "completely new objects and instances\n",
            "\n",
            "which we as humans may not have seen or\n",
            "\n",
            "even thought about right parts of the\n",
            "\n",
            "design space that are not covered by the\n",
            "\n",
            "training data so an example here is this\n",
            "\n",
            "AI generated art which art if you will\n",
            "\n",
            "right which was created by a diffusion\n",
            "\n",
            "model\n",
            "\n",
            "and I think not only does this get at\n",
            "\n",
            "some of the limits and capabilities of\n",
            "\n",
            "these powerful models but also questions\n",
            "\n",
            "about what does it mean to create new\n",
            "\n",
            "instances what are the limits and Bounds\n",
            "\n",
            "of these models and how do they how can\n",
            "\n",
            "we think about their advances with\n",
            "\n",
            "respect to human capabilities and human\n",
            "\n",
            "intelligence\n",
            "\n",
            "and so I'm I'm really excited that on\n",
            "\n",
            "Thursday in lecture seven on New\n",
            "\n",
            "Frontiers in deep learning we're going\n",
            "\n",
            "to take a really deep dive into\n",
            "\n",
            "diffusion models talk about their\n",
            "\n",
            "fundamentals talk about not only\n",
            "\n",
            "applications to images but other fields\n",
            "\n",
            "as well in which we're seeing these\n",
            "\n",
            "models really start to make a\n",
            "\n",
            "transformative advances because they are\n",
            "\n",
            "indeed at the very Cutting Edge and very\n",
            "\n",
            "much the New Frontier of generative AI\n",
            "\n",
            "today\n",
            "\n",
            "all right so with that\n",
            "\n",
            "tease and and and\n",
            "\n",
            "um hopefully set the stage for lecture\n",
            "\n",
            "seven on Thursday and\n",
            "\n",
            "conclude and remind you all that we have\n",
            "\n",
            "now about an hour for open Office hour\n",
            "\n",
            "time for you to work on your software\n",
            "\n",
            "Labs come to us ask any questions you\n",
            "\n",
            "may have as well as the Tas who will be\n",
            "\n",
            "here as well thank you so much\n",
            "\n",
            "[Applause]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}